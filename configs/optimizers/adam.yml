# optimizer
optimizer: adam
learning_rate: 0.0001
weight_decay: 0.0001

# Scheduler
lr_scheduler: steplr
lr_step_size: 1
lr_gamma: 0.97
